{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5aeaee0",
   "metadata": {},
   "source": [
    "Set up a Python development environment for Azure Machine Learning <br>\n",
    "<a href = \"https://docs.microsoft.com/en-in/azure/machine-learning/how-to-configure-environment#local\">Set up development environment</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130ec89",
   "metadata": {},
   "source": [
    "Train a model with azure Machine Learning <br><a href = \"https://docs.microsoft.com/en-in/azure/machine-learning/tutorial-train-models-with-aml\">Train a model with azure ML</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f46297ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mThe default web browser has been opened at https://login.microsoftonline.com/organizations/oauth2/v2.0/authorize. Please continue the login in the web browser. If no web browser is available or if the web browser fails to open, use device code flow with `az login --use-device-code`.\u001b[0m\n",
      "Opening in existing browser session.\n",
      "[17962:17962:0100/000000.820984:ERROR:sandbox_linux.cc(374)] InitializeSandbox() called with multiple threads in process gpu-process.\n",
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"fb44495d-8da7-44d4-b597-6199eb799ccc\",\n",
      "    \"id\": \"b1997ed0-2373-4622-83fc-102035f13ae5\",\n",
      "    \"isDefault\": true,\n",
      "    \"managedByTenants\": [],\n",
      "    \"name\": \"Azure for Students\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"fb44495d-8da7-44d4-b597-6199eb799ccc\",\n",
      "    \"user\": {\n",
      "      \"name\": \"bhavyakumawat99@gmail.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "! az login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a277b5",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12ed77bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.36.0\n"
     ]
    }
   ],
   "source": [
    "# Import Python packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c61625",
   "metadata": {},
   "source": [
    "### Create workspace resources you need to get started with Azure Machine Learning\n",
    "<a href = \"https://docs.microsoft.com/en-in/azure/machine-learning/quickstart-create-resources\">Workspace resources</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43f64d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The resource group doesn't exist or was not provided. AzureML SDK is creating a resource group=Mask-Detection in location=eastus using subscription=b1997ed0-2373-4622-83fc-102035f13ae5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying AppInsights with name maskdeteinsights9a1cb6c6.\n",
      "Deployed AppInsights with name maskdeteinsights9a1cb6c6. Took 8.21 seconds.\n",
      "Deploying StorageAccount with name maskdetestorage15e951a21.\n",
      "Deploying KeyVault with name maskdetekeyvaultcbf87f24.\n",
      "Deployed KeyVault with name maskdetekeyvaultcbf87f24. Took 24.85 seconds.\n",
      "Deployed StorageAccount with name maskdetestorage15e951a21. Took 29.07 seconds.\n",
      "Deploying Workspace with name mask-detector.\n",
      "Deployed Workspace with name mask-detector. Took 28.42 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Create the workspace\n",
    "\n",
    "ws = Workspace.create(name='mask-detector',\n",
    "               subscription_id='b1997ed0-2373-4622-83fc-102035f13ae5',\n",
    "               resource_group='Mask-Detection',\n",
    "               create_resource_group=True,\n",
    "               location='eastus' # VM size - Standard_NC6 is only available in East U.S. and South Central U.S. locations\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f6a0a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask-detector\teastus\tMask-Detection\n"
     ]
    }
   ],
   "source": [
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca901acf",
   "metadata": {},
   "source": [
    "### Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96fed858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment to track the runs in your workspace.\n",
    "experiment_name = 'mask_detection_experiments'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede2c80",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7deb9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new compute target...\n",
      "InProgress...\n",
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Resizing', 'allocationStateTransitionTime': '2021-11-19T06:54:08.022000+00:00', 'errors': None, 'creationTime': '2021-11-19T06:54:07.583227+00:00', 'modifiedTime': '2021-11-19T06:54:23.215307+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "# By using Azure Machine Learning Compute, you can train machine learning models on clusters of Azure virtual machines.\n",
    "# create Azure Machine Learning Compute as your training environment\n",
    "\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = \"cpu-cluster\"\n",
    "compute_min_nodes = 0\n",
    "compute_max_nodes = 4\n",
    "\n",
    "# This example uses CPU VM, set SKU to STANDARD_D2_V2. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = \"STANDARD_NC6\"\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3508b",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71b256",
   "metadata": {},
   "source": [
    "* A storage account, blob storage containers and a Datastore in ML workspace is already created for you by Azure ML service\n",
    "\n",
    "\n",
    "* There are two dataset types that can be used in Azure Machine Learning training workflows : FileDatasets and TabularDatasets. <br>\n",
    "<a href= \"https://docs.microsoft.com/en-in/azure/machine-learning/how-to-create-register-datasets#create-a-filedataset\">Create a FileDataset with the Python SDK or the Azure Machine Learning studio</a> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to upload all the files from a local directory, create a FileDataset in a single method with \n",
    "# upload_directory(). This method uploads data to your underlying storage, and as a result incur storage costs.\n",
    "\n",
    "from azureml.core import Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "datastore = ws.get_default_datastore() # Datastore.get(ws, 'workspaceblobstore')\n",
    "src_dir = os.path.join(os.getcwd(), 'resized_Dataset') \n",
    "\n",
    "# upload directory\n",
    "ds = datastore.upload(src_dir, target_path='/dataset', overwrite=False, show_progress=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab2d7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create FileDataset object\n",
    "datastore_paths = (datastore, '/dataset/**')\n",
    "mask_ds = Dataset.File.from_files(path=datastore_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29987eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you register the Dataset to your workspace for easy retrieval during training.\n",
    "mask_dataset = mask_ds.register(workspace=ws,\n",
    "                                name='mask_dataset',\n",
    "                                create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed762601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_dataset = Dataset.get_by_name(ws, name='mask_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d0b97",
   "metadata": {},
   "source": [
    "### Train on a remote cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18d28a",
   "metadata": {},
   "source": [
    "For this task, you submit the job to run on the remote training cluster you set up earlier. To submit a job you:\n",
    "* Create a directory\n",
    "* Create a training script\n",
    "* Create a script run configuration\n",
    "* Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03561f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to deliver the necessary code from your computer to the remote resource.\n",
    "script_folder = os.path.join(os.getcwd(), \"Mask_Detection_Directory\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df6d97",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e3e43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = os.path.join(os.getcwd(), \"Mask_Detection_Directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36dd58a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /usr/local/bin/machine_learning_projects/iNeuron/Face_Mask_Detector/Mask_Detection_Directory/train.py\n"
     ]
    }
   ],
   "source": [
    "%run ./Training_Script.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ffb97",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on. Configure the ScriptRunConfig by specifying:\n",
    "\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution.\n",
    "* The compute target. In this case, you use the Azure Machine Learning compute cluster you created.\n",
    "* The training script name, train.py.\n",
    "* An environment that contains the libraries needed to run the script.\n",
    "* Arguments required from the training script.\n",
    "\n",
    "\n",
    "<a href = \"https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-tensorflow#set-up-the-experiment\"> More on curated environments</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8152839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Azure ML provides prebuilt, curated environments if you don't want to define your own environment.\n",
    "curated_env_name = 'AzureML-TensorFlow-2.2-GPU'\n",
    "env = Environment.get(workspace=ws, name=curated_env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24e37013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the packages included in the curated environment\n",
    "env.save_to_directory(path=curated_env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40123281",
   "metadata": {},
   "source": [
    "If the curated environment does not includes all the dependencies required by your training script, you will have to modify the environment to include the missing dependencies. And if the environment is modified, you will have to give it a new name, as the 'AzureML' prefix is reserved for curated environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c41f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_conda_specification(name='mask-env', file_path='./AzureML-TensorFlow-2.2-GPU/conda_dependencies.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95653e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ScriptRunConfig by specifying the training script, compute target and environment.\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "args = ['--data-folder', mask_dataset.as_mount(), '--training-lr', 1e-04, '--training-epochs', 10,\n",
    "        '--fine-tuning-lr', 1e-05, '--fine-tuning-epochs', 5]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8093f",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c413df10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>mask_detection_experiments</td><td>mask_detection_experiments_1637311987_1e2318e8</td><td>azureml.scriptrun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/mask_detection_experiments_1637311987_1e2318e8?wsid=/subscriptions/b1997ed0-2373-4622-83fc-102035f13ae5/resourcegroups/Mask-Detection/workspaces/mask-detector&amp;tid=fb44495d-8da7-44d4-b597-6199eb799ccc\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: mask_detection_experiments,\n",
       "Id: mask_detection_experiments_1637311987_1e2318e8,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the experiment by submitting the ScriptRunConfig object\n",
    "# the call is asynchronous, it returns a Preparing or Running state as soon as the job is started\n",
    "run = exp.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd47c26",
   "metadata": {},
   "source": [
    "### Monitor a remote run\n",
    "In total, the first run takes about 10 minutes. But for subsequent runs, as long as the script dependencies don't change, the same image is reused. So the container startup time is much faster.\n",
    "\n",
    "What happens while you wait:\n",
    "\n",
    "* Image creation: A Docker image is created that matches the Python environment specified by the Azure ML environment. The image is uploaded to the workspace. Image creation and uploading takes about five minutes.\n",
    "\n",
    "* This stage happens once for each Python environment because the container is cached for subsequent runs. During image creation, logs are streamed to the run history. You can monitor the image creation progress by using these logs.\n",
    "\n",
    "* Scaling: If the remote cluster requires more nodes to do the run than currently available, additional nodes are added automatically. Scaling typically takes about five minutes.\n",
    "\n",
    "* Running: In this stage, the necessary scripts and files are sent to the compute target. Then datastores are mounted or copied. And then the entry_script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. You can monitor the run's progress by using these logs.\n",
    "\n",
    "* Post-processing: The ./outputs directory of the run is copied over to the run history in your workspace, so you can access these results.\n",
    "\n",
    "You can check the progress of a running job in several ways. This tutorial uses a Jupyter widget and a wait_for_completion method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d506f3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9510d6cf6ca4825807c79ec7d6dd3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/mask_detection_experiments_1637311987_1e2318e8?wsid=/subscriptions/b1997ed0-2373-4622-83fc-102035f13ae5/resourcegroups/Mask-Detection/workspaces/mask-detector&tid=fb44495d-8da7-44d4-b597-6199eb799ccc\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"run_properties\": {\"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"created_utc\": \"2021-11-19T08:53:15.167456Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"c7e8af1f-8ba1-4761-813f-d59041fb1624\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":1}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2021-11-19T10:14:22.668278Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://maskdetestorage15e951a21.blob.core.windows.net/azureml/ExperimentRun/dcid.mask_detection_experiments_1637311987_1e2318e8/azureml-logs/20_image_build_log.txt?sv=2019-07-07&sr=b&sig=94HR54GgexFg2bEmDd%2FRVs%2FKu6xYsnV1sc6AqP4rjc8%3D&skoid=b1f664ac-aacc-4461-b5b0-ca664c0106a1&sktid=fb44495d-8da7-44d4-b597-6199eb799ccc&skt=2021-11-19T08%3A11%3A44Z&ske=2021-11-20T16%3A21%3A44Z&sks=b&skv=2019-07-07&st=2021-11-19T10%3A25%3A12Z&se=2021-11-19T18%3A35%3A12Z&sp=r\", \"logs/azureml/19_azureml.log\": \"https://maskdetestorage15e951a21.blob.core.windows.net/azureml/ExperimentRun/dcid.mask_detection_experiments_1637311987_1e2318e8/logs/azureml/19_azureml.log?sv=2019-07-07&sr=b&sig=a81mPRHKwe%2B%2BdGZ2TSr9yz3ByWyFCq7%2Br0ovReRaJZU%3D&skoid=b1f664ac-aacc-4461-b5b0-ca664c0106a1&sktid=fb44495d-8da7-44d4-b597-6199eb799ccc&skt=2021-11-19T08%3A11%3A44Z&ske=2021-11-20T16%3A21%3A44Z&sks=b&skv=2019-07-07&st=2021-11-19T10%3A25%3A12Z&se=2021-11-19T18%3A35%3A12Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/19_azureml.log\"], [\"azureml-logs/20_image_build_log.txt\"]], \"run_duration\": \"1:21:07\", \"run_number\": \"3\", \"run_queued_details\": {\"status\": \"Completed\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"training learning rate\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"categories\": [0], \"series\": [{\"data\": [0.0001]}]}, {\"name\": \"training epochs\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"categories\": [0], \"series\": [{\"data\": [10.0]}]}, {\"name\": \"fine tuning learning rate\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"categories\": [0], \"series\": [{\"data\": [1e-05]}]}, {\"name\": \"fine tuning epochs\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"categories\": [0], \"series\": [{\"data\": [5.0]}]}, {\"name\": \"accuracy\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"categories\": [0], \"series\": [{\"data\": [\"array([0.96973091, 0.99047083, 0.99019057, 0.9955157 , 0.99327356])\"]}]}, {\"name\": \"val_accuracy\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"categories\": [0], \"series\": [{\"data\": [\"array([0.953125 , 0.9776786, 0.9921875, 0.9910714, 0.9921875])\"]}]}, {\"name\": \"loss\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"categories\": [0], \"series\": [{\"data\": [\"array([0.07360248, 0.02582102, 0.0216311 , 0.01282537, 0.01763402])\"]}]}, {\"name\": \"val_loss\", \"run_id\": \"mask_detection_experiments_1637311987_1e2318e8\", \"categories\": [0], \"series\": [{\"data\": [\"array([0.08866281, 0.06216094, 0.02249909, 0.01635629, 0.0238467 ])\"]}]}], \"run_logs\": \"2021/11/19 08:53:19 Downloading source code...\\r\\n2021/11/19 08:53:20 Finished downloading source code\\r\\n2021/11/19 08:53:20 Creating Docker network: acb_default_network, driver: 'bridge'\\n2021/11/19 08:53:20 Successfully set up Docker network: acb_default_network\\n2021/11/19 08:53:20 Setting up Docker configuration...\\n2021/11/19 08:53:21 Successfully set up Docker configuration\\n2021/11/19 08:53:21 Logging in to registry: 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io\\n2021/11/19 08:53:22 Successfully logged into 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io\\n2021/11/19 08:53:22 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\\n2021/11/19 08:53:22 Scanning for dependencies...\\n2021/11/19 08:53:22 Successfully scanned dependencies\\n2021/11/19 08:53:22 Launching container with name: acb_step_0\\r\\nSending build context to Docker daemon  66.56kB\\r\\r\\nStep 1/19 : FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211029.v1@sha256:fda0c9dd1702f56aa5ce42ebeeda6af6b768d1cb18c04380af3ded4184eedb09\\nmcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211029.v1@sha256:fda0c9dd1702f56aa5ce42ebeeda6af6b768d1cb18c04380af3ded4184eedb09: Pulling from azureml/openmpi3.1.2-ubuntu18.04\\r\\nDigest: sha256:fda0c9dd1702f56aa5ce42ebeeda6af6b768d1cb18c04380af3ded4184eedb09\\nStatus: Downloaded newer image for mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211029.v1@sha256:fda0c9dd1702f56aa5ce42ebeeda6af6b768d1cb18c04380af3ded4184eedb09\\n ---> 02c62b48927a\\nStep 2/19 : USER root\\n ---> Running in f3520ab55248\\nRemoving intermediate container f3520ab55248\\n ---> 7ac711114a90\\nStep 3/19 : RUN mkdir -p $HOME/.cache\\n ---> Running in 29069fef0410\\nRemoving intermediate container 29069fef0410\\n ---> 0ffdfe27c251\\nStep 4/19 : WORKDIR /\\r\\n ---> Running in 88ed47831506\\nRemoving intermediate container 88ed47831506\\n ---> 4bc2d45294b0\\nStep 5/19 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\\n ---> 5dc8365b6328\\nStep 6/19 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\\n ---> Running in 5b0e0762b880\\nRemoving intermediate container 5b0e0762b880\\n ---> da8a1f1be377\\nStep 7/19 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\\r\\n ---> da05b9a6d850\\nStep 8/19 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \\\"$HOME/.cache/pip\\\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \\\"$CONDA_ROOT_DIR/pkgs\\\" && find \\\"$CONDA_ROOT_DIR\\\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\\n ---> Running in d3ab622322e7\\nWarning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\\r\\nCollecting package metadata (repodata.json): ...working... \\r\\ndone\\r\\nSolving environment: ...working... done\\r\\n\\nDownloading and Extracting Packages\\n\\r_libgcc_mutex-0.1    | 3 KB      |            |   0% \\r_libgcc_mutex-0.1    | 3 KB      | ########## | 100% \\n\\rca-certificates-2021 | 139 KB    |            |   0% \\rca-certificates-2021 | 139 KB    | ########## | 100% \\n\\r_openmp_mutex-4.5    | 22 KB     |            |   0% \\r_openmp_mutex-4.5    | 22 KB     | ########## | 100% \\n\\ropenssl-1.0.2u       | 3.2 MB    |            |   0% \\ropenssl-1.0.2u       | 3.2 MB    | ########## | 100% \\ropenssl-1.0.2u       | 3.2 MB    | ########## | 100% \\n\\rsetuptools-58.0.4    | 966 KB    |            |   0% \\rsetuptools-58.0.4    | 966 KB    | ########## | 100% \\rsetuptools-58.0.4    | 966 KB    | ########## | 100% \\n\\rtk-8.5.19            | 1.9 MB    |            |   0% \\rtk-8.5.19            | 1.9 MB    | ########## | 100% \\rtk-8.5.19            | 1.9 MB    | ########## | 100% \\n\\rlibzlib-1.2.11       | 59 KB     |            |   0% \\rlibzlib-1.2.11       | 59 KB     | ########## | 100% \\n\\rpython-3.6.2         | 19.0 MB   |            |   0% \\rpython-3.6.2         | 19.0 MB   | ####3      |  44% \\rpython-3.6.2         | 19.0 MB   | ########## | 100% \\rpython-3.6.2         | 19.0 MB   | ########## | 100% \\r\\n\\rpip-21.3.1           | 1.2 MB    |            |   0% \\rpip-21.3.1           | 1.2 MB    | ########## | 100% \\rpip-21.3.1           | 1.2 MB    | ########## | 100% \\n\\rzlib-1.2.11          | 86 KB     |            |   0% \\rzlib-1.2.11          | 86 KB     | ########## | 100% \\n\\rxz-5.2.5             | 343 KB    |            |   0% \\rxz-5.2.5             | 343 KB    | ########## | 100% \\n\\rlibgomp-11.2.0       | 427 KB    |            |   0% \\rlibgomp-11.2.0       | 427 KB    | ########## | 100% \\n\\rpython_abi-3.6       | 4 KB      |            |   0% \\rpython_abi-3.6       | 4 KB      | ########## | 100% \\n\\rlibgcc-ng-11.2.0     | 887 KB    |            |   0% \\rlibgcc-ng-11.2.0     | 887 KB    | ########## | 100% \\rlibgcc-ng-11.2.0     | 887 KB    | ########## | 100% \\n\\rncurses-5.9          | 1.1 MB    |            |   0% \\rncurses-5.9          | 1.1 MB    | ########## | 100% \\rncurses-5.9          | 1.1 MB    | ########## | 100% \\n\\rsqlite-3.13.0        | 4.9 MB    |            |   0% \\rsqlite-3.13.0        | 4.9 MB    | ########## | 100% \\rsqlite-3.13.0        | 4.9 MB    | ########## | 100% \\n\\rwheel-0.37.0         | 31 KB     |            |   0% \\rwheel-0.37.0         | 31 KB     | ########## | 100% \\n\\rreadline-6.2         | 713 KB    |            |   0% \\rreadline-6.2         | 713 KB    | ########## | 100% \\rreadline-6.2         | 713 KB    | ########## | 100% \\nPreparing transaction: ...working... done\\r\\nVerifying transaction: ...working... done\\nExecuting transaction: ...working... done\\r\\nInstalling pip dependencies: ...working... \\r\\nRan pip subprocess with arguments:\\n['/azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e/bin/python', '-m', 'pip', 'install', '-U', '-r', '/azureml-environment-setup/condaenv.fjk3p0b_.requirements.txt']\\nPip subprocess output:\\nCollecting azureml-core==1.35.0\\n  Downloading azureml_core-1.35.0-py3-none-any.whl (2.2 MB)\\nCollecting azureml-defaults==1.35.0\\n  Downloading azureml_defaults-1.35.0-py3-none-any.whl (3.1 kB)\\nCollecting azureml-telemetry==1.35.0\\n  Downloading azureml_telemetry-1.35.0-py3-none-any.whl (30 kB)\\nCollecting azureml-train-restclients-hyperdrive==1.35.0\\n  Downloading azureml_train_restclients_hyperdrive-1.35.0-py3-none-any.whl (19 kB)\\nCollecting azureml-train-core==1.35.0\\n  Downloading azureml_train_core-1.35.0-py3-none-any.whl (8.6 MB)\\nCollecting tensorflow-gpu==2.2.0\\n  Downloading tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\\nCollecting horovod==0.19.5\\n  Downloading horovod-0.19.5.tar.gz (2.9 MB)\\n  Preparing metadata (setup.py): started\\n  Preparing metadata (setup.py): finished with status 'done'\\nCollecting pillow\\n  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\\nCollecting joblib\\n  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\\nCollecting contextlib2<22.0.0\\n  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\\nCollecting msrestazure<=0.6.4,>=0.4.33\\n  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\\nCollecting adal<=1.2.7,>=1.2.0\\n  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\\nCollecting ruamel.yaml<=0.17.16,>=0.17.10\\n  Downloading ruamel.yaml-0.17.16-py3-none-any.whl (109 kB)\\nCollecting urllib3<=1.26.6,>=1.23\\n  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\\nCollecting pytz\\n  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\\nCollecting azure-mgmt-resource<15.0.0,>=1.2.1\\n  Downloading azure_mgmt_resource-13.0.0-py2.py3-none-any.whl (1.3 MB)\\nCollecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*,<4.0.0\\n  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\\nCollecting msrest<1.0.0,>=0.5.1\\n  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\\nCollecting azure-mgmt-keyvault<10.0.0,>=0.40.0\\n  Downloading azure_mgmt_keyvault-9.3.0-py2.py3-none-any.whl (412 kB)\\nCollecting docker<6.0.0\\n  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\\nCollecting PyJWT<3.0.0\\n  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\\nCollecting python-dateutil<3.0.0,>=2.7.3\\n  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\\nCollecting azure-mgmt-authorization<1.0.0,>=0.40.0\\n  Downloading azure_mgmt_authorization-0.61.0-py2.py3-none-any.whl (94 kB)\\nCollecting pyopenssl<21.0.0\\n  Downloading pyOpenSSL-20.0.1-py2.py3-none-any.whl (54 kB)\\nCollecting jmespath<1.0.0\\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\\nCollecting azure-graphrbac<1.0.0,>=0.40.0\\n  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\\nCollecting azure-common<2.0.0,>=1.1.12\\n  Downloading azure_common-1.1.27-py2.py3-none-any.whl (12 kB)\\nCollecting requests<3.0.0,>=2.19.1\\n  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\\nCollecting pathspec<1.0.0\\n  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\\nCollecting SecretStorage<4.0.0\\n  Downloading SecretStorage-3.3.1-py3-none-any.whl (15 kB)\\nCollecting jsonpickle<3.0.0\\n  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\\nCollecting ndg-httpsclient<=0.5.1\\n  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\\nCollecting azure-mgmt-containerregistry>=2.0.0\\n  Downloading azure_mgmt_containerregistry-8.2.0-py2.py3-none-any.whl (928 kB)\\nCollecting azure-mgmt-storage<16.0.0,>=1.5.0\\n  Downloading azure_mgmt_storage-11.2.0-py2.py3-none-any.whl (547 kB)\\nCollecting backports.tempfile\\n  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\\nCollecting opencensus-ext-azure==1.0.8\\n  Downloading opencensus_ext_azure-1.0.8-py2.py3-none-any.whl (35 kB)\\nCollecting azureml-dataset-runtime[fuse]~=1.35.0\\n  Downloading azureml_dataset_runtime-1.35.0-py3-none-any.whl (3.5 kB)\\nCollecting configparser==3.7.4\\r\\n  Downloading configparser-3.7.4-py2.py3-none-any.whl (22 kB)\\nCollecting gunicorn==20.1.0\\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\\nCollecting json-logging-py==0.2\\n  Downloading json-logging-py-0.2.tar.gz (3.6 kB)\\n  Preparing metadata (setup.py): started\\n  Preparing metadata (setup.py): finished with status 'done'\\nCollecting werkzeug<=1.0.1,>=0.16.1\\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\\nCollecting azureml-inference-server-http~=0.3.1\\n  Downloading azureml_inference_server_http-0.3.2-py3-none-any.whl (38 kB)\\nCollecting applicationinsights\\n  Downloading applicationinsights-0.11.10-py2.py3-none-any.whl (55 kB)\\nCollecting tensorboard<2.3.0,>=2.2.0\\n  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\\nCollecting google-pasta>=0.1.8\\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\\nCollecting wrapt>=1.11.1\\n  Downloading wrapt-1.13.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (78 kB)\\nCollecting absl-py>=0.7.0\\n  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\\nCollecting opt-einsum>=2.3.2\\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\\nRequirement already satisfied: wheel>=0.26 in /azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e/lib/python3.6/site-packages (from tensorflow-gpu==2.2.0->-r /azureml-environment-setup/condaenv.fjk3p0b_.requirements.txt (line 6)) (0.37.0)\\nCollecting numpy<2.0,>=1.16.0\\n  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\\nCollecting grpcio>=1.8.6\\n  Downloading grpcio-1.42.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\\nCollecting scipy==1.4.1\\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\\nCollecting astunparse==1.6.3\\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\\nCollecting keras-preprocessing>=1.1.0\\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\\nCollecting termcolor>=1.1.0\\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\\n  Preparing metadata (setup.py): started\\n  Preparing metadata (setup.py): finished with status 'done'\\nCollecting six>=1.12.0\\n  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\\nCollecting h5py<2.11.0,>=2.10.0\\n  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\\nCollecting protobuf>=3.8.0\\n  Downloading protobuf-3.19.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\\nCollecting tensorflow-estimator<2.3.0,>=2.2.0\\n  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\\nCollecting gast==0.3.3\\n  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\\nCollecting cloudpickle\\n  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\\nCollecting psutil\\n  Downloading psutil-5.8.0-cp36-cp36m-manylinux2010_x86_64.whl (291 kB)\\nCollecting pyyaml\\n  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\\nCollecting cffi>=1.4.0\\n  Downloading cffi-1.15.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (405 kB)\\nRequirement already satisfied: setuptools>=3.0 in /azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e/lib/python3.6/site-packages (from gunicorn==20.1.0->azureml-defaults==1.35.0->-r /azureml-environment-setup/condaenv.fjk3p0b_.requirements.txt (line 2)) (58.0.4)\\nCollecting opencensus<1.0.0,>=0.7.13\\n  Downloading opencensus-0.8.0-py2.py3-none-any.whl (128 kB)\\nCollecting azure-mgmt-core<2.0.0,>=1.2.0\\n  Downloading azure_mgmt_core-1.3.0-py2.py3-none-any.whl (25 kB)\\nCollecting pyarrow<4.0.0,>=0.17.0\\n  Downloading pyarrow-3.0.0-cp36-cp36m-manylinux2014_x86_64.whl (20.7 MB)\\nCollecting azureml-dataprep<2.24.0a,>=2.23.0a\\n  Downloading azureml_dataprep-2.23.2-py3-none-any.whl (39.4 MB)\\nCollecting fusepy<4.0.0,>=3.0.1\\n  Downloading fusepy-3.0.1.tar.gz (11 kB)\\n  Preparing metadata (setup.py): started\\n  Preparing metadata (setup.py): finished with status 'done'\\nCollecting inference-schema==1.3.0\\n  Downloading inference_schema-1.3.0-py3-none-any.whl (19 kB)\\nCollecting flask==1.0.3\\n  Downloading Flask-1.0.3-py2.py3-none-any.whl (92 kB)\\nCollecting Jinja2>=2.10\\n  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\\nCollecting itsdangerous>=0.24\\n  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\\nCollecting click>=5.1\\n  Downloading click-8.0.3-py3-none-any.whl (97 kB)\\nCollecting wrapt>=1.11.1\\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\\n  Preparing metadata (setup.py): started\\n  Preparing metadata (setup.py): finished with status 'done'\\nCollecting pycparser\\n  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\\nCollecting websocket-client>=0.32.0\\n  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\\nCollecting importlib-metadata\\n  Downloading importlib_metadata-4.8.2-py3-none-any.whl (17 kB)\\nCollecting isodate>=0.6.0\\n  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\\nCollecting certifi>=2017.4.17\\n  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\\nCollecting requests-oauthlib>=0.5.0\\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\\nCollecting pyasn1>=0.1.1\\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\\nCollecting idna<4,>=2.5\\n  Downloading idna-3.3-py3-none-any.whl (61 kB)\\nCollecting charset-normalizer~=2.0.0\\n  Downloading charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\\nCollecting ruamel.yaml.clib>=0.1.2\\n  Downloading ruamel.yaml.clib-0.2.6-cp36-cp36m-manylinux1_x86_64.whl (552 kB)\\nCollecting jeepney>=0.6\\n  Downloading jeepney-0.7.1-py3-none-any.whl (54 kB)\\nCollecting google-auth<2,>=1.6.3\\n  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\\nCollecting tensorboard-plugin-wit>=1.6.0\\n  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\\nCollecting markdown>=2.6.8\\n  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\\nCollecting google-auth-oauthlib<0.5,>=0.4.1\\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\\nCollecting backports.weakref\\n  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\\nCollecting azure-core<2.0.0,>=1.15.0\\n  Downloading azure_core-1.20.1-py2.py3-none-any.whl (177 kB)\\nCollecting azure-identity<1.5.0,>=1.2.0\\n  Downloading azure_identity-1.4.1-py2.py3-none-any.whl (86 kB)\\nCollecting azureml-dataprep-rslex~=1.21.0dev0\\n  Downloading azureml_dataprep_rslex-1.21.2-cp36-cp36m-manylinux2010_x86_64.whl (13.0 MB)\\nCollecting cloudpickle\\n  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\\nCollecting azureml-dataprep-native<39.0.0,>=38.0.0\\n  Downloading azureml_dataprep_native-38.0.0-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\\nCollecting dotnetcore2<3.0.0,>=2.1.14\\n  Downloading dotnetcore2-2.1.21-py3-none-manylinux1_x86_64.whl (28.7 MB)\\nCollecting pyasn1-modules>=0.2.1\\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\\nCollecting cachetools<5.0,>=2.0.0\\n  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\\nCollecting rsa<5,>=3.1.4\\n  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\\nCollecting zipp>=0.5\\n  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\\nCollecting typing-extensions>=3.6.4\\n  Downloading typing_extensions-4.0.0-py3-none-any.whl (22 kB)\\nCollecting opencensus-context==0.1.2\\n  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\\nCollecting google-api-core<3.0.0,>=1.0.0\\n  Downloading google_api_core-2.2.2-py2.py3-none-any.whl (95 kB)\\nCollecting contextvars\\n  Downloading contextvars-2.4.tar.gz (9.6 kB)\\n  Preparing metadata (setup.py): started\\n  Preparing metadata (setup.py): finished with status 'done'\\nCollecting oauthlib>=3.0.0\\n  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\\nCollecting msal<2.0.0,>=1.3.0\\n  Downloading msal-1.16.0-py2.py3-none-any.whl (78 kB)\\nCollecting msal-extensions~=0.2.2\\n  Downloading msal_extensions-0.2.2-py2.py3-none-any.whl (15 kB)\\nCollecting distro>=1.2.0\\n  Downloading distro-1.6.0-py2.py3-none-any.whl (19 kB)\\nCollecting googleapis-common-protos<2.0dev,>=1.52.0\\n  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\\nCollecting MarkupSafe>=2.0\\n  Downloading MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\\nCollecting portalocker~=1.0\\n  Downloading portalocker-1.7.1-py2.py3-none-any.whl (10 kB)\\nCollecting immutables>=0.9\\n  Downloading immutables-0.16-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\\nBuilding wheels for collected packages: horovod, json-logging-py, termcolor, wrapt, fusepy, contextvars\\n  Building wheel for horovod (setup.py): started\\n  Building wheel for horovod (setup.py): finished with status 'error'\\n  Running setup.py clean for horovod\\n  Building wheel for json-logging-py (setup.py): started\\n  Building wheel for json-logging-py (setup.py): finished with status 'done'\\n  Created wheel for json-logging-py: filename=json_logging_py-0.2-py3-none-any.whl size=3925 sha256=27cb84376de58455148df77b63502166a8d4d7f8c36b3a8651d46e0d86814d43\\n  Stored in directory: /root/.cache/pip/wheels/e2/1d/52/535a274b9c2ce7d4064838f2bdb62013801281ef7d7f21e2ee\\n  Building wheel for termcolor (setup.py): started\\n  Building wheel for termcolor (setup.py): finished with status 'done'\\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=e46fafe8ad75e1b26a3ea368e4182f424cd6adba6f33f779727f7a4f7a26da81\\n  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\\n  Building wheel for wrapt (setup.py): started\\n  Building wheel for wrapt (setup.py): finished with status 'done'\\n  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69942 sha256=9b0a4fc3a1111623c95d2edf7ad7b2ddb5cc2b20cdb4fec721a46e5fb8ceca9f\\n  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\\n  Building wheel for fusepy (setup.py): started\\n  Building wheel for fusepy (setup.py): finished with status 'done'\\n  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10502 sha256=b93d33385ddb8e08533f84c3025bc113a6453ba1ffee00c66ce2345009bc7ec4\\n  Stored in directory: /root/.cache/pip/wheels/21/5c/83/1dd7e8a232d12227e5410120f4374b33adeb4037473105b079\\n  Building wheel for contextvars (setup.py): started\\n  Building wheel for contextvars (setup.py): finished with status 'done'\\n  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7680 sha256=a37f93d3badb37b6a18c700b609ad3446a5003c7c2ee5dafd6c356b0181972f5\\n  Stored in directory: /root/.cache/pip/wheels/41/11/53/911724983aa48deb94792432e14e518447212dd6c5477d49d3\\nSuccessfully built json-logging-py termcolor wrapt fusepy contextvars\\nFailed to build horovod\\nInstalling collected packages: pycparser, cffi, urllib3, PyJWT, idna, cryptography, charset-normalizer, certifi, six, requests, oauthlib, typing-extensions, requests-oauthlib, python-dateutil, pyasn1, portalocker, msal, isodate, zipp, rsa, pyasn1-modules, protobuf, msrest, msal-extensions, immutables, distro, cachetools, azure-core, adal, websocket-client, ruamel.yaml.clib, pyopenssl, numpy, msrestazure, MarkupSafe, jeepney, importlib-metadata, googleapis-common-protos, google-auth, dotnetcore2, contextvars, cloudpickle, backports.weakref, azureml-dataprep-rslex, azureml-dataprep-native, azure-mgmt-core, azure-identity, azure-common, wrapt, werkzeug, SecretStorage, ruamel.yaml, pytz, pyarrow, pathspec, opencensus-context, ndg-httpsclient, jsonpickle, jmespath, Jinja2, itsdangerous, google-api-core, docker, contextlib2, click, backports.tempfile, azureml-dataprep, azure-mgmt-storage, azure-mgmt-resource, azure-mgmt-keyvault, azure-mgmt-containerregistry, azure-mgmt-authorization, azure-graphrbac, tensorboard-plugin-wit, psutil, opencensus, markdown, inference-schema, gunicorn, grpcio, google-auth-oauthlib, fusepy, flask, azureml-dataset-runtime, azureml-core, applicationinsights, absl-py, termcolor, tensorflow-estimator, tensorboard, scipy, pyyaml, opt-einsum, opencensus-ext-azure, keras-preprocessing, json-logging-py, h5py, google-pasta, gast, configparser, azureml-train-restclients-hyperdrive, azureml-telemetry, azureml-inference-server-http, astunparse, tensorflow-gpu, pillow, joblib, horovod, azureml-train-core, azureml-defaults\\n    Running setup.py install for horovod: started\\n    Running setup.py install for horovod: still running...\\n    Running setup.py install for horovod: finished with status 'done'\\nSuccessfully installed Jinja2-3.0.3 MarkupSafe-2.0.1 PyJWT-2.3.0 SecretStorage-3.3.1 absl-py-1.0.0 adal-1.2.7 applicationinsights-0.11.10 astunparse-1.6.3 azure-common-1.1.27 azure-core-1.20.1 azure-graphrbac-0.61.1 azure-identity-1.4.1 azure-mgmt-authorization-0.61.0 azure-mgmt-containerregistry-8.2.0 azure-mgmt-core-1.3.0 azure-mgmt-keyvault-9.3.0 azure-mgmt-resource-13.0.0 azure-mgmt-storage-11.2.0 azureml-core-1.35.0 azureml-dataprep-2.23.2 azureml-dataprep-native-38.0.0 azureml-dataprep-rslex-1.21.2 azureml-dataset-runtime-1.35.0 azureml-defaults-1.35.0 azureml-inference-server-http-0.3.2 azureml-telemetry-1.35.0 azureml-train-core-1.35.0 azureml-train-restclients-hyperdrive-1.35.0 backports.tempfile-1.0 backports.weakref-1.0.post1 cachetools-4.2.4 certifi-2021.10.8 cffi-1.15.0 charset-normalizer-2.0.7 click-8.0.3 cloudpickle-1.6.0 configparser-3.7.4 contextlib2-21.6.0 contextvars-2.4 cryptography-3.4.8 distro-1.6.0 docker-5.0.3 dotnetcore2-2.1.21 flask-1.0.3 fusepy-3.0.1 gast-0.3.3 google-api-core-2.2.2 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 googleapis-common-protos-1.53.0 grpcio-1.42.0 gunicorn-20.1.0 h5py-2.10.0 horovod-0.19.5 idna-3.3 immutables-0.16 importlib-metadata-4.8.2 inference-schema-1.3.0 isodate-0.6.0 itsdangerous-2.0.1 jeepney-0.7.1 jmespath-0.10.0 joblib-1.1.0 json-logging-py-0.2 jsonpickle-2.0.0 keras-preprocessing-1.1.2 markdown-3.3.6 msal-1.16.0 msal-extensions-0.2.2 msrest-0.6.21 msrestazure-0.6.4 ndg-httpsclient-0.5.1 numpy-1.19.5 oauthlib-3.1.1 opencensus-0.8.0 opencensus-context-0.1.2 opencensus-ext-azure-1.0.8 opt-einsum-3.3.0 pathspec-0.9.0 pillow-8.4.0 portalocker-1.7.1 protobuf-3.19.1 psutil-5.8.0 pyarrow-3.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 pyopenssl-20.0.1 python-dateutil-2.8.2 pytz-2021.3 pyyaml-6.0 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 ruamel.yaml-0.17.16 ruamel.yaml.clib-0.2.6 scipy-1.4.1 six-1.16.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.8.0 tensorflow-estimator-2.2.0 tensorflow-gpu-2.2.0 termcolor-1.1.0 typing-extensions-4.0.0 urllib3-1.26.6 websocket-client-1.2.1 werkzeug-1.0.1 wrapt-1.12.1 zipp-3.6.0\\n\\ndone\\n#\\n# To activate this environment, use\\n#\\n#     $ conda activate /azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e\\n#\\n# To deactivate an active environment, use\\n#\\n#     $ conda deactivate\\n\\n\\u001b[91m\\n\\n==> WARNING: A newer version of conda exists. <==\\n  current version: 4.9.2\\n  latest version: 4.10.3\\n\\nPlease update conda by running\\n\\n    $ conda update -n base -c defaults conda\\n\\n\\r\\n\\u001b[0mWARNING: /root/.conda/pkgs does not exist\\r\\nRemoving intermediate container d3ab622322e7\\n ---> e9ba33dbcaa0\\nStep 9/19 : ENV PATH /azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e/bin:$PATH\\r\\n ---> Running in 15c5fa3c205e\\nRemoving intermediate container 15c5fa3c205e\\n ---> 01b7e471d7ed\\nStep 10/19 : COPY azureml-environment-setup/send_conda_dependencies.py azureml-environment-setup/send_conda_dependencies.py\\r\\n ---> a1a516c8d644\\nStep 11/19 : RUN echo \\\"Copying environment context\\\"\\r\\n ---> Running in f79466212281\\nCopying environment context\\nRemoving intermediate container f79466212281\\n ---> e621a51bfb16\\nStep 12/19 : COPY azureml-environment-setup/environment_context.json azureml-environment-setup/environment_context.json\\r\\n ---> 259b10c2ad70\\nStep 13/19 : RUN python /azureml-environment-setup/send_conda_dependencies.py -p /azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e\\r\\n ---> Running in 9ec1ff15dd67\\nReport materialized dependencies for the environment\\nReading environment context\\nExporting conda environment\\nSending request with materialized conda environment details\\nSuccessfully sent materialized environment details\\r\\nRemoving intermediate container 9ec1ff15dd67\\n ---> 1e166eff2ecf\\nStep 14/19 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e\\r\\n ---> Running in 8d543017c7ce\\nRemoving intermediate container 8d543017c7ce\\n ---> b58570e76384\\nStep 15/19 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_76bcf556a73eff9220f818aea0b17e7e/lib:$LD_LIBRARY_PATH\\r\\n ---> Running in bd301e344f8e\\nRemoving intermediate container bd301e344f8e\\n ---> 8f0c459e498e\\nStep 16/19 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\\r\\n ---> 5bf7b04cd497\\nStep 17/19 : RUN if [ $SPARK_HOME ]; then /bin/bash -c '$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py'; fi\\r\\n ---> Running in a6a92280e9d9\\nRemoving intermediate container a6a92280e9d9\\n ---> f4ef50dc0843\\nStep 18/19 : ENV AZUREML_ENVIRONMENT_IMAGE True\\r\\n ---> Running in fba20f121352\\nRemoving intermediate container fba20f121352\\n ---> acd99b524478\\nStep 19/19 : CMD [\\\"bash\\\"]\\r\\n ---> Running in e513d6d09078\\nRemoving intermediate container e513d6d09078\\n ---> aa97d4576005\\nSuccessfully built aa97d4576005\\nSuccessfully tagged 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io/azureml/azureml_047748c5b9de1d8c50d0a35053b48c01:latest\\r\\nSuccessfully tagged 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io/azureml/azureml_047748c5b9de1d8c50d0a35053b48c01:1\\n2021/11/19 08:57:31 Successfully executed container: acb_step_0\\n2021/11/19 08:57:31 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\\n2021/11/19 08:57:31 Pushing image: 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io/azureml/azureml_047748c5b9de1d8c50d0a35053b48c01:1, attempt 1\\nThe push refers to repository [921b98f2647d4cbeb286f8bb921ad01d.azurecr.io/azureml/azureml_047748c5b9de1d8c50d0a35053b48c01]\\n8d44df269b45: Preparing\\n7046e5e11ffd: Preparing\\n5ced59e17591: Preparing\\nd749a3b64e07: Preparing\\n7de22ef3e3a5: Preparing\\n9c6fc3e3dc7d: Preparing\\n1add18237e7f: Preparing\\n4fcf32603aa5: Preparing\\n139344e56bfd: Preparing\\n1f5c3b6b406d: Preparing\\nd4e24c9af7a3: Preparing\\n7191ffd7c8d7: Preparing\\nd19cbe53fe3d: Preparing\\n16c731a63aa1: Preparing\\ndc5b6ac7528b: Preparing\\n9d681d299f1a: Preparing\\nc7297795e901: Preparing\\n824bf068fd3d: Preparing\\n1f5c3b6b406d: Waiting\\nd4e24c9af7a3: Waiting\\n7191ffd7c8d7: Waiting\\nd19cbe53fe3d: Waiting\\n16c731a63aa1: Waiting\\ndc5b6ac7528b: Waiting\\n9d681d299f1a: Waiting\\nc7297795e901: Waiting\\n824bf068fd3d: Waiting\\n1add18237e7f: Waiting\\n4fcf32603aa5: Waiting\\n139344e56bfd: Waiting\\n9c6fc3e3dc7d: Waiting\\n5ced59e17591: Pushed\\nd749a3b64e07: Pushed\\n8d44df269b45: Pushed\\n7046e5e11ffd: Pushed\\n9c6fc3e3dc7d: Pushed\\r\\n4fcf32603aa5: Pushed\\n1add18237e7f: Pushed\\n139344e56bfd: Pushed\\n1f5c3b6b406d: Pushed\\nd4e24c9af7a3: Pushed\\r\\nd19cbe53fe3d: Pushed\\n7191ffd7c8d7: Pushed\\r\\ndc5b6ac7528b: Pushed\\r\\n9d681d299f1a: Pushed\\r\\n824bf068fd3d: Pushed\\r\\n16c731a63aa1: Pushed\\n\\r\\nc7297795e901: Pushed\\r\\n7de22ef3e3a5: Pushed\\r\\n1: digest: sha256:776a7eb4a0db40abe24c5f5a3f2c1ababc88144405395fe4f6da4ee3d9b3a5a6 size: 4099\\n2021/11/19 08:59:27 Successfully pushed image: 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io/azureml/azureml_047748c5b9de1d8c50d0a35053b48c01:1\\n2021/11/19 08:59:27 Executing step ID: acb_step_2. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\\n2021/11/19 08:59:27 Pushing image: 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io/azureml/azureml_047748c5b9de1d8c50d0a35053b48c01:latest, attempt 1\\nThe push refers to repository [921b98f2647d4cbeb286f8bb921ad01d.azurecr.io/azureml/azureml_047748c5b9de1d8c50d0a35053b48c01]\\n8d44df269b45: Preparing\\n7046e5e11ffd: Preparing\\n5ced59e17591: Preparing\\nd749a3b64e07: Preparing\\n7de22ef3e3a5: Preparing\\n9c6fc3e3dc7d: Preparing\\n1add18237e7f: Preparing\\n4fcf32603aa5: Preparing\\n139344e56bfd: Preparing\\n1f5c3b6b406d: Preparing\\nd4e24c9af7a3: Preparing\\n7191ffd7c8d7: Preparing\\nd19cbe53fe3d: Preparing\\n16c731a63aa1: Preparing\\ndc5b6ac7528b: Preparing\\n9d681d299f1a: Preparing\\nc7297795e901: Preparing\\n824bf068fd3d: Preparing\\nd4e24c9af7a3: Waiting\\n7191ffd7c8d7: Waiting\\nd19cbe53fe3d: Waiting\\n16c731a63aa1: Waiting\\n9c6fc3e3dc7d: Waiting\\ndc5b6ac7528b: Waiting\\n1add18237e7f: Waiting\\n4fcf32603aa5: Waiting\\n139344e56bfd: Waiting\\n1f5c3b6b406d: Waiting\\n9d681d299f1a: Waiting\\n824bf068fd3d: Waiting\\nc7297795e901: Waiting\\n5ced59e17591: Layer already exists\\n7de22ef3e3a5: Layer already exists\\n7046e5e11ffd: Layer already exists\\nd749a3b64e07: Layer already exists\\n9c6fc3e3dc7d: Layer already exists\\n8d44df269b45: Layer already exists\\r\\n1add18237e7f: Layer already exists\\n139344e56bfd: Layer already exists\\n1f5c3b6b406d: Layer already exists\\n4fcf32603aa5: Layer already exists\\ndc5b6ac7528b: Layer already exists\\r\\n7191ffd7c8d7: Layer already exists\\nd4e24c9af7a3: Layer already exists\\n16c731a63aa1: Layer already exists\\nd19cbe53fe3d: Layer already exists\\n824bf068fd3d: Layer already exists\\n9d681d299f1a: Layer already exists\\nc7297795e901: Layer already exists\\nlatest: digest: sha256:776a7eb4a0db40abe24c5f5a3f2c1ababc88144405395fe4f6da4ee3d9b3a5a6 size: 4099\\n2021/11/19 08:59:33 Successfully pushed image: 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io/azureml/azureml_047748c5b9de1d8c50d0a35053b48c01:latest\\n2021/11/19 08:59:33 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 249.098038)\\n2021/11/19 08:59:33 Populating digests for step ID: acb_step_0...\\n2021/11/19 08:59:34 Successfully populated digests for step ID: acb_step_0\\n2021/11/19 08:59:34 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 115.781997)\\n2021/11/19 08:59:34 Step ID: acb_step_2 marked as successful (elapsed time in seconds: 6.300473)\\n2021/11/19 08:59:34 The following dependencies were found:\\n2021/11/19 08:59:34 \\n- image:\\n    registry: 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io\\n    repository: azureml/azureml_047748c5b9de1d8c50d0a35053b48c01\\n    tag: latest\\n    digest: sha256:776a7eb4a0db40abe24c5f5a3f2c1ababc88144405395fe4f6da4ee3d9b3a5a6\\n  runtime-dependency:\\n    registry: mcr.microsoft.com\\n    repository: azureml/openmpi3.1.2-ubuntu18.04\\n    tag: 20211029.v1\\n    digest: sha256:fda0c9dd1702f56aa5ce42ebeeda6af6b768d1cb18c04380af3ded4184eedb09\\n  git: {}\\n- image:\\n    registry: 921b98f2647d4cbeb286f8bb921ad01d.azurecr.io\\n    repository: azureml/azureml_047748c5b9de1d8c50d0a35053b48c01\\n    tag: \\\"1\\\"\\n    digest: sha256:776a7eb4a0db40abe24c5f5a3f2c1ababc88144405395fe4f6da4ee3d9b3a5a6\\n  runtime-dependency:\\n    registry: mcr.microsoft.com\\n    repository: azureml/openmpi3.1.2-ubuntu18.04\\n    tag: 20211029.v1\\n    digest: sha256:fda0c9dd1702f56aa5ce42ebeeda6af6b768d1cb18c04380af3ded4184eedb09\\n  git: {}\\n\\n\\r\\nRun ID: ca2 was successful after 6m16s\\r\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.36.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Jupyter widget\n",
    "# Watch the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous \n",
    "# and provides live updates every 10 to 15 seconds until the job finishes\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44d5cc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'mask_detection_experiments_1637311987_1e2318e8',\n",
       " 'target': 'cpu-cluster',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2021-11-19T09:03:24.824647Z',\n",
       " 'endTimeUtc': '2021-11-19T10:14:22.668278Z',\n",
       " 'services': {},\n",
       " 'warnings': [{'message': 'This run might be using a new job runtime with improved performance and error reporting. The logs from your script are in user_logs/std_log.txt. Please let us know if you run into any issues, and if you would like to opt-out, please add the environment variable AZUREML_COMPUTE_USE_COMMON_RUNTIME to the environment variables section of the job and set its value to the string \"false\"'}],\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': 'c7e8af1f-8ba1-4761-813f-d59041fb1624',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '3b7f846b-a4f1-44fa-96f4-6a4647f66299'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'input__3b7f846b', 'mechanism': 'Mount'}}],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'command': '',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data-folder',\n",
       "   'DatasetConsumptionConfig:input__3b7f846b',\n",
       "   '--training-lr',\n",
       "   '0.0001',\n",
       "   '--training-epochs',\n",
       "   '10',\n",
       "   '--fine-tuning-lr',\n",
       "   '1E-05',\n",
       "   '--fine-tuning-epochs',\n",
       "   '5'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'cpu-cluster',\n",
       "  'dataReferences': {},\n",
       "  'data': {'input__3b7f846b': {'dataLocation': {'dataset': {'id': '3b7f846b-a4f1-44fa-96f4-6a4647f66299',\n",
       "      'name': 'mask_dataset',\n",
       "      'version': '1'},\n",
       "     'dataPath': None,\n",
       "     'uri': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'input__3b7f846b',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False,\n",
       "    'options': None}},\n",
       "  'outputData': {},\n",
       "  'datacaches': [],\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': 2592000,\n",
       "  'nodeCount': 1,\n",
       "  'instanceTypes': [],\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'mask-env',\n",
       "   'version': 'Autosave_2021-11-19T08:53:14Z_292e15c6',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-core==1.35.0',\n",
       "        'azureml-defaults==1.35.0',\n",
       "        'azureml-telemetry==1.35.0',\n",
       "        'azureml-train-restclients-hyperdrive==1.35.0',\n",
       "        'azureml-train-core==1.35.0',\n",
       "        'tensorflow-gpu==2.2.0',\n",
       "        'horovod==0.19.5',\n",
       "        'pillow',\n",
       "        'joblib']}],\n",
       "     'name': 'azureml_76bcf556a73eff9220f818aea0b17e7e'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211029.v1',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': None},\n",
       "  'aiSuperComputer': {'instanceType': 'D2',\n",
       "   'imageVersion': 'pytorch-1.7.0',\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None,\n",
       "   'virtualClusterArmId': None,\n",
       "   'tensorboardLogDirectory': None,\n",
       "   'sshPublicKey': None,\n",
       "   'sshPublicKeys': None,\n",
       "   'enableAzmlInt': True,\n",
       "   'priority': 'Medium',\n",
       "   'slaTier': 'Standard',\n",
       "   'userAlias': None},\n",
       "  'kubernetesCompute': {'instanceType': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'pyTorch': {'communicationBackend': 'nccl', 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': False,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {},\n",
       "  'applicationEndpoints': {},\n",
       "  'parameters': []},\n",
       " 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://maskdetestorage15e951a21.blob.core.windows.net/azureml/ExperimentRun/dcid.mask_detection_experiments_1637311987_1e2318e8/azureml-logs/20_image_build_log.txt?sv=2019-07-07&sr=b&sig=r1j46JCSIbhHzCgHBmU5ex5IL3OKMCcxgaohPRGOKXI%3D&skoid=b1f664ac-aacc-4461-b5b0-ca664c0106a1&sktid=fb44495d-8da7-44d4-b597-6199eb799ccc&skt=2021-11-19T08%3A11%3A44Z&ske=2021-11-20T16%3A21%3A44Z&sks=b&skv=2019-07-07&st=2021-11-19T10%3A02%3A02Z&se=2021-11-19T18%3A12%3A02Z&sp=r',\n",
       "  'logs/azureml/19_azureml.log': 'https://maskdetestorage15e951a21.blob.core.windows.net/azureml/ExperimentRun/dcid.mask_detection_experiments_1637311987_1e2318e8/logs/azureml/19_azureml.log?sv=2019-07-07&sr=b&sig=o41T%2B%2BDDxnZmBw0OkKrGTIZs6hNAZHMs94nAyHLXU68%3D&skoid=b1f664ac-aacc-4461-b5b0-ca664c0106a1&sktid=fb44495d-8da7-44d4-b597-6199eb799ccc&skt=2021-11-19T08%3A11%3A44Z&ske=2021-11-20T16%3A21%3A44Z&sks=b&skv=2019-07-07&st=2021-11-19T10%3A02%3A02Z&se=2021-11-19T18%3A12%3A02Z&sp=r'},\n",
       " 'submittedBy': 'Bhavya Kumawat'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f9314b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training learning rate': 0.0001, 'training epochs': 10.0, 'fine tuning learning rate': 1e-05, 'fine tuning epochs': 5.0, 'accuracy': 'array([0.96973091, 0.99047083, 0.99019057, 0.9955157 , 0.99327356])', 'val_accuracy': 'array([0.953125 , 0.9776786, 0.9921875, 0.9910714, 0.9921875])', 'loss': 'array([0.07360248, 0.02582102, 0.0216311 , 0.01282537, 0.01763402])', 'val_loss': 'array([0.08866281, 0.06216094, 0.02249909, 0.01635629, 0.0238467 ])'}\n"
     ]
    }
   ],
   "source": [
    "# Display run results\n",
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04424a47",
   "metadata": {},
   "source": [
    "### Register model\n",
    "\n",
    "Outputs is a special directory in that all content in this directory is automatically uploaded to your workspace. This content appears in the run record in the experiment under your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58ea105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_detection_model\tmask_detection_model:1\t1\n"
     ]
    }
   ],
   "source": [
    "# register model\n",
    "model = run.register_model(model_name='mask_detection_model',\n",
    "                           model_path='outputs/1')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40d866",
   "metadata": {},
   "source": [
    "### Download model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea40346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "run.download_files(prefix='outputs/my_model.h5', output_directory='./model', append_prefix=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
